{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdf30d5-207e-4a72-9153-53c34e4a1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from KDEpy import NaiveKDE\n",
    "import scipy\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bin.dataset import Dataset\n",
    "from bin.experiment import Experiment\n",
    "from bin.metrics import Metrics\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from models.LR import Lr\n",
    "from models.reduction import Reduction\n",
    "from models.reweight import Reweight\n",
    "from models.fair_reduction import FairReduction\n",
    "\n",
    "from scipy.special import xlog1py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, plot_model_comparison,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,true_positive_rate, \n",
    "    equalized_odds_difference)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86838275-55a5-485d-9842-3ac88c7c87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the largest available float on the system\n",
    "try:\n",
    "    FLOAT = scipy.float128\n",
    "except AttributeError:\n",
    "    FLOAT = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395d88cb-3c52-4791-ad78-02c717133e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    \"\"\"Reads the config file and returns a dictionary.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found.\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads the csv file and returns a dataframe.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found.\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fabcb856-31b5-4a2f-8753-4f78ba17647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_conf = \"configs/adult_noisy.json\"\n",
    "exp_conf = \"configs/COMPAS_noisy.json\"\n",
    "#exp_conf = \"configs/synthetic_20_noisy.json\"\n",
    "#exp_conf = \"configs/income_noisy.json\"\n",
    "# exp_conf = \"configs/baseline_config.json\"\n",
    "\n",
    "EXP = read_config(exp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e68011e8-1a17-4359-b8b4-ab33792b943b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['COMPAS_bias_0.1', 'COMPAS_bias_0.3', 'COMPAS_flip_0.1', 'COMPAS_flip_0.3', 'COMPAS_balanced_0.1', 'COMPAS_balanced_0.3', 'COMPAS_bias_0.1_cov', 'COMPAS_bias_0.3_cov', 'COMPAS_flip_0.1_cov', 'COMPAS_flip_0.3_cov', 'COMPAS_balanced_0.1_cov', 'COMPAS_balanced_0.3_cov'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fe27ec2-9a77-463f-828a-f541a1e3a511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/probabilities/FairProbabilities/COMPAS/balanced/0.1/10/para/max-ll/predict-per-example-proxy-label.csv not found\n",
      "self.name='COMPAS' Key error in fold='x10', split_type='train' and prob_name='prob'\n",
      "assets/probabilities/FairProbabilities/COMPAS/balanced/0.1/10/para/max-ll/predict-per-example-proxy-label.csv not found\n",
      "self.name='COMPAS' Key error in fold='x10', split_type='train' and prob_name='prob'\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for name, value in EXP.items():\n",
    "    datasets[name] = Dataset(value)\n",
    "    try:\n",
    "    # if name in ['adult_bias_0.1','adult_bias_0.3']: continue\n",
    "    # if name in ['COMPAS_balanced_0.1']: continue\n",
    "    # if not name in ['income_balanced_0.1', 'income_balanced_0.3']: continue\n",
    "    # if name  not in ['synthetic_20_balanced_0.1', 'synthetic_20_balanced_0.3']: continue\n",
    "        continue\n",
    "    # datasets[name].calculate_probabilities()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86912a4-28cf-4ee0-8dac-3b81ad984aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPAS_bias_0.1': <bin.dataset.Dataset at 0x7f08dfccfe90>,\n",
       " 'COMPAS_bias_0.3': <bin.dataset.Dataset at 0x7f08e4a35110>,\n",
       " 'COMPAS_flip_0.1': <bin.dataset.Dataset at 0x7f08dfb104d0>,\n",
       " 'COMPAS_flip_0.3': <bin.dataset.Dataset at 0x7f08dfbe4e50>,\n",
       " 'COMPAS_balanced_0.1': <bin.dataset.Dataset at 0x7f08dfb61a10>,\n",
       " 'COMPAS_balanced_0.3': <bin.dataset.Dataset at 0x7f08e4979150>,\n",
       " 'COMPAS_bias_0.1_cov': <bin.dataset.Dataset at 0x7f08e48cd650>,\n",
       " 'COMPAS_bias_0.3_cov': <bin.dataset.Dataset at 0x7f08dfc3f210>,\n",
       " 'COMPAS_flip_0.1_cov': <bin.dataset.Dataset at 0x7f08dfccbc50>,\n",
       " 'COMPAS_flip_0.3_cov': <bin.dataset.Dataset at 0x7f08dfcccdd0>,\n",
       " 'COMPAS_balanced_0.1_cov': <bin.dataset.Dataset at 0x7f08e43f1a50>,\n",
       " 'COMPAS_balanced_0.3_cov': <bin.dataset.Dataset at 0x7f08e47675d0>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_name = 'COMPAS_balanced_0.1_cov'\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0ff8f1c-ba64-4b18-a9f3-c924efb627ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_name = 'COMPAS_flip_0.3'\n",
    "splits_path = 'data/covariate_shifted_data/compas/flip/0.3/'\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "valid_ids = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    fold = 'x' + str(i)\n",
    "    exclude = ['prob', 'emp_prob', EXP[dset_name]['sensitive_attribute'], EXP[dset_name]['label']]\n",
    "    \n",
    "    len_train = len(datasets[dset_name].foldwise_data[fold]['train'])\n",
    "    len_test = len(datasets[dset_name].foldwise_data[fold]['test'])\n",
    "    len_valid = len(datasets[dset_name].foldwise_data[fold]['valid'])\n",
    "    \n",
    "    dataX = datasets[dset_name].data\n",
    "    dataX = dataX[list(set(dataX.columns) - set(exclude))]\n",
    "\n",
    "    tr_idxs, test_idxs = create_shift(dataX, (len_train) / (len_train + len_test + len_valid), holdout=0.4, alpha=1, beta=2)\n",
    "\n",
    "    omitted = []\n",
    "    for i in range(max(max(tr_idxs), max(test_idxs))):\n",
    "        if i not in tr_idxs and i not in test_idxs:\n",
    "            omitted.append(i)\n",
    "    \n",
    "    tr_data = dataX.iloc[tr_idxs]\n",
    "    test_data = dataX.iloc[test_idxs]\n",
    "    unused_data = dataX.iloc[omitted]\n",
    "    \n",
    "    train_ids.append(tr_idxs)\n",
    "    test_ids.append(test_idxs[:int(len(test_idxs) / 2)])\n",
    "    valid_ids.append(test_idxs[int(len(test_idxs) / 2):])\n",
    "\n",
    "new_train_ids = pd.DataFrame(train_ids).transpose()\n",
    "new_train_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_train_ids.to_csv(splits_path + 'train_ids.csv', index=False)\n",
    "\n",
    "new_test_ids = pd.DataFrame(test_ids).transpose()\n",
    "new_test_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_test_ids.to_csv(splits_path + 'test_ids.csv', index=False)\n",
    "\n",
    "new_valid_ids = pd.DataFrame(valid_ids).transpose()\n",
    "new_valid_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_valid_ids.to_csv(splits_path + 'valid_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "542dbb35-8d0b-459d-961e-e96e037d98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_distance(data1, data2):\n",
    "    data = pd.concat([data1, data2], axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    mean1 = np.mean(pc[:len(data1)])\n",
    "    std1 = np.std(pc[:len(data1)])\n",
    "    mean2 = np.mean(pc[len(data1):len(data1)+len(data2)])\n",
    "    std2 = np.std(pc[len(data1):len(data1)+len(data2)])\n",
    "\n",
    "    return mean1, std1, mean2, std2\n",
    "\n",
    "def create_shift(\n",
    "    data,\n",
    "    src_split=0.8,\n",
    "    holdout=0.2,\n",
    "    alpha=1,\n",
    "    beta=2,\n",
    "    kdebw=0.3,\n",
    "    eps=0.001,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates covariate shift sampling of data into disjoint source and target set.\n",
    "\n",
    "    Let \\mu and \\sigma be the mean and the standard deviation of the first principal component retrieved by PCA on the whole data.\n",
    "    The target is randomly sampled based on a Gaussian with mean = \\mu and standard deviation = \\sigma.\n",
    "    The source is randomly sampled based on a Gaussian with mean = \\mu + alpha and standard devaition = \\sigma / beta\n",
    "\n",
    "    data: [m, n]\n",
    "    alpha, beta: the parameter that distorts the gaussian used in sampling\n",
    "                   according to the first principle component\n",
    "    output: source indices, target indices, ratios based on kernel density estimation with bandwidth = kdebw and smoothed by eps\n",
    "    \"\"\"\n",
    "    m = np.shape(data)[0]\n",
    "    source_size = int(m * src_split * (1 - holdout))\n",
    "    target_size = int(m * (1 - src_split) * (1 - holdout))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    pc_mean = np.mean(pc)\n",
    "    pc_std = np.std(pc)\n",
    "\n",
    "    sample_mean = pc_mean + alpha\n",
    "    sample_std = pc_std / beta\n",
    "\n",
    "    # sample according to the probs\n",
    "    prob_s = norm.pdf(pc, loc=sample_mean, scale=sample_std)\n",
    "    sum_s = np.sum(prob_s)\n",
    "    prob_s = prob_s / sum_s\n",
    "    prob_t = norm.pdf(pc, loc=pc_mean, scale=pc_std)\n",
    "    sum_t = np.sum(prob_t)\n",
    "    prob_t = prob_t / sum_t\n",
    "\n",
    "    source_ind = np.random.choice(\n",
    "        range(m), size=source_size, replace=False, p=np.reshape(prob_s, (m))\n",
    "    )\n",
    "\n",
    "    pt_proxy = np.copy(prob_t)\n",
    "    pt_proxy[source_ind] = 0\n",
    "    pt_proxy = pt_proxy / np.sum(pt_proxy)\n",
    "    target_ind = np.random.choice(\n",
    "        range(m), size=target_size, replace=False, p=np.reshape(pt_proxy, (m))\n",
    "    )\n",
    "\n",
    "    #assert np.all(np.sort(source_ind) != np.sort(target_ind))\n",
    "    #src_kde = KDEAdapter(kde=NaiveKDE(kernel=\"gaussian\", bw=kdebw)).fit(\n",
    "    #    pc2[source_ind, :]\n",
    "    #)\n",
    "    #trg_kde = KDEAdapter(kde=NaiveKDE(kernel=\"gaussian\", bw=kdebw)).fit(\n",
    "    #    pc2[target_ind, :]\n",
    "    #)\n",
    "\n",
    "    #ratios = src_kde.p(pc2, eps) / trg_kde.p(pc2, eps)\n",
    "    #print(\"min ratio= {:.5f}, max ratio= {:.5f}\".format(np.min(ratios), np.max(ratios)))\n",
    "\n",
    "    return source_ind, target_ind#, ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4b288-f257-474c-843a-e6d1f1913237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
