{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdf30d5-207e-4a72-9153-53c34e4a1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from KDEpy import NaiveKDE\n",
    "import scipy\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bin.dataset import Dataset\n",
    "from bin.experiment import Experiment\n",
    "from bin.metrics import Metrics\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from models.LR import Lr\n",
    "from models.reduction import Reduction\n",
    "from models.reweight import Reweight\n",
    "from models.fair_reduction import FairReduction\n",
    "\n",
    "from scipy.special import xlog1py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, plot_model_comparison,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,true_positive_rate, \n",
    "    equalized_odds_difference)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86838275-55a5-485d-9842-3ac88c7c87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the largest available float on the system\n",
    "try:\n",
    "    FLOAT = scipy.float128\n",
    "except AttributeError:\n",
    "    FLOAT = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395d88cb-3c52-4791-ad78-02c717133e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    \"\"\"Reads the config file and returns a dictionary.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found.\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads the csv file and returns a dataframe.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found.\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabcb856-31b5-4a2f-8753-4f78ba17647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_conf = \"configs/adult_noisy.json\"\n",
    "#exp_conf = \"configs/COMPAS_noisy.json\"\n",
    "#exp_conf = \"configs/synthetic_20_noisy.json\"\n",
    "exp_conf = \"configs/income_noisy.json\"\n",
    "# exp_conf = \"configs/baseline_config.json\"\n",
    "\n",
    "EXP = read_config(exp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68011e8-1a17-4359-b8b4-ab33792b943b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['income_bias_0.1', 'income_bias_0.3', 'income_flip_0.1', 'income_flip_0.3', 'income_balanced_0.1', 'income_balanced_0.3'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe27ec2-9a77-463f-828a-f541a1e3a511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for name, value in EXP.items():\n",
    "    datasets[name] = Dataset(value)\n",
    "    try:\n",
    "    # if name in ['adult_bias_0.1','adult_bias_0.3']: continue\n",
    "    # if name in ['COMPAS_balanced_0.1']: continue\n",
    "    # if not name in ['income_balanced_0.1', 'income_balanced_0.3']: continue\n",
    "    # if name  not in ['synthetic_20_balanced_0.1', 'synthetic_20_balanced_0.3']: continue\n",
    "        continue\n",
    "    # datasets[name].calculate_probabilities()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86912a4-28cf-4ee0-8dac-3b81ad984aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'income_bias_0.1': <bin.dataset.Dataset at 0x7f3dce4da110>,\n",
       " 'income_bias_0.3': <bin.dataset.Dataset at 0x7f3dcd720b10>,\n",
       " 'income_flip_0.1': <bin.dataset.Dataset at 0x7f3dcd61a550>,\n",
       " 'income_flip_0.3': <bin.dataset.Dataset at 0x7f3dcd48fbd0>,\n",
       " 'income_balanced_0.1': <bin.dataset.Dataset at 0x7f3dcd51b990>,\n",
       " 'income_balanced_0.3': <bin.dataset.Dataset at 0x7f3dcd3a77d0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_name = 'income_balanced_0.1_cov'\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0ff8f1c-ba64-4b18-a9f3-c924efb627ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dset_name = 'income_flip_0.3'\n",
    "splits_path = 'data/covariate_shifted_data/0.25_1.1/income/flip/0.3/'\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "valid_ids = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    fold = 'x' + str(i)\n",
    "    exclude = ['prob', 'emp_prob', EXP[dset_name]['sensitive_attribute'], EXP[dset_name]['label']]\n",
    "    \n",
    "    len_train = len(datasets[dset_name].foldwise_data[fold]['train'])\n",
    "    len_test = len(datasets[dset_name].foldwise_data[fold]['test'])\n",
    "    len_valid = len(datasets[dset_name].foldwise_data[fold]['valid'])\n",
    "    \n",
    "    dataX = datasets[dset_name].data\n",
    "    dataX = dataX[list(set(dataX.columns) - set(exclude))]\n",
    "\n",
    "    tr_idxs, test_idxs = create_shift(dataX, (len_train + len_valid) / (len_train + len_test + len_valid), holdout=0.4, alpha=0.25, beta=1.1)\n",
    "\n",
    "    omitted = []\n",
    "    for i in range(max(max(tr_idxs), max(test_idxs))):\n",
    "        if i not in tr_idxs and i not in test_idxs:\n",
    "            omitted.append(i)\n",
    "    \n",
    "    tr_data = dataX.iloc[tr_idxs]\n",
    "    test_data = dataX.iloc[test_idxs]\n",
    "    unused_data = dataX.iloc[omitted]\n",
    "    \n",
    "    train_ids.append(tr_idxs[:int(len(tr_idxs) * len_train / (len_train + len_valid))])\n",
    "    test_ids.append(test_idxs)\n",
    "    valid_ids.append(tr_idxs[int(len(tr_idxs) * len_train / (len_train + len_valid)):])\n",
    "\n",
    "new_train_ids = pd.DataFrame(train_ids).transpose()\n",
    "new_train_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_train_ids.to_csv(splits_path + 'train_ids.csv', index=False)\n",
    "\n",
    "new_test_ids = pd.DataFrame(test_ids).transpose()\n",
    "new_test_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_test_ids.to_csv(splits_path + 'test_ids.csv', index=False)\n",
    "\n",
    "new_valid_ids = pd.DataFrame(valid_ids).transpose()\n",
    "new_valid_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "new_valid_ids.to_csv(splits_path + 'valid_ids.csv', index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d608949-366c-4d70-8193-81c5b5872d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195659"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(train_ids[i]) for i in range(len(train_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "542dbb35-8d0b-459d-961e-e96e037d98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_distance(data1, data2):\n",
    "    data = pd.concat([data1, data2], axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    mean1 = np.mean(pc[:len(data1)])\n",
    "    std1 = np.std(pc[:len(data1)])\n",
    "    mean2 = np.mean(pc[len(data1):len(data1)+len(data2)])\n",
    "    std2 = np.std(pc[len(data1):len(data1)+len(data2)])\n",
    "\n",
    "    return mean1, std1, mean2, std2\n",
    "\n",
    "def create_shift(\n",
    "    data,\n",
    "    src_split=0.8,\n",
    "    holdout=0.2,\n",
    "    alpha=1,\n",
    "    beta=2,\n",
    "    kdebw=0.3,\n",
    "    eps=0.001,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates covariate shift sampling of data into disjoint source and target set.\n",
    "\n",
    "    Let \\mu and \\sigma be the mean and the standard deviation of the first principal component retrieved by PCA on the whole data.\n",
    "    The target is randomly sampled based on a Gaussian with mean = \\mu and standard deviation = \\sigma.\n",
    "    The source is randomly sampled based on a Gaussian with mean = \\mu + alpha and standard devaition = \\sigma / beta\n",
    "\n",
    "    data: [m, n]\n",
    "    alpha, beta: the parameter that distorts the gaussian used in sampling\n",
    "                   according to the first principle component\n",
    "    output: source indices, target indices, ratios based on kernel density estimation with bandwidth = kdebw and smoothed by eps\n",
    "    \"\"\"\n",
    "    m = np.shape(data)[0]\n",
    "    source_size = int(m * src_split * (1 - holdout))\n",
    "    target_size = int(m * (1 - src_split) * (1 - holdout))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    pc_mean = np.mean(pc)\n",
    "    pc_std = np.std(pc)\n",
    "\n",
    "    sample_mean = pc_mean + alpha\n",
    "    sample_std = pc_std / beta\n",
    "\n",
    "    # sample according to the probs\n",
    "    prob_s = norm.pdf(pc, loc=sample_mean, scale=sample_std)\n",
    "    sum_s = np.sum(prob_s)\n",
    "    prob_s = prob_s / sum_s\n",
    "    prob_t = norm.pdf(pc, loc=pc_mean, scale=pc_std)\n",
    "    sum_t = np.sum(prob_t)\n",
    "    prob_t = prob_t / sum_t\n",
    "\n",
    "    source_ind = np.random.choice(\n",
    "        range(m), size=source_size, replace=False, p=np.reshape(prob_s, (m))\n",
    "    )\n",
    "\n",
    "    pt_proxy = np.copy(prob_t)\n",
    "    pt_proxy[source_ind] = 0\n",
    "    pt_proxy = pt_proxy / np.sum(pt_proxy)\n",
    "    target_ind = np.random.choice(\n",
    "        range(m), size=target_size, replace=False, p=np.reshape(pt_proxy, (m))\n",
    "    )\n",
    "\n",
    "    return source_ind, target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4b288-f257-474c-843a-e6d1f1913237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
