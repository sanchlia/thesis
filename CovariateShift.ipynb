{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdf30d5-207e-4a72-9153-53c34e4a1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from KDEpy import NaiveKDE\n",
    "import scipy\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bin.dataset import Dataset\n",
    "from bin.experiment import Experiment\n",
    "#from bin.metrics import Metrics\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "#from models.LR import Lr\n",
    "#from models.reduction import Reduction\n",
    "#from models.reweight import Reweight\n",
    "#from models.fair_reduction import FairReduction\n",
    "\n",
    "from scipy.special import xlog1py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score\n",
    "#from fairlearn.metrics import (\n",
    "#    MetricFrame, plot_model_comparison,\n",
    "#    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "#    false_positive_rate, false_negative_rate,\n",
    "#    false_positive_rate_difference, false_negative_rate_difference,true_positive_rate, \n",
    "#    equalized_odds_difference)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86838275-55a5-485d-9842-3ac88c7c87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the largest available float on the system\n",
    "try:\n",
    "    FLOAT = scipy.float128\n",
    "except AttributeError:\n",
    "    FLOAT = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395d88cb-3c52-4791-ad78-02c717133e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    \"\"\"Reads the config file and returns a dictionary.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found.\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads the csv file and returns a dataframe.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found.\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fabcb856-31b5-4a2f-8753-4f78ba17647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_conf = \"configs/adult_noisy.json\"\n",
    "#exp_conf = \"configs/COMPAS_noisy.json\"\n",
    "#exp_conf = \"configs/synthetic_20_noisy.json\"\n",
    "exp_conf = \"configs/income_noisy.json\"\n",
    "# exp_conf = \"configs/baseline_config.json\"\n",
    "\n",
    "EXP = read_config(exp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bf753c3f-2968-447b-89a0-073ad8495acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['income_bias_0.1', 'income_bias_0.3', 'income_flip_0.1', 'income_flip_0.3', 'income_balanced_0.1', 'income_balanced_0.3'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP2 = EXP.copy()\n",
    "for k in EXP2.keys():\n",
    "    if 'cov' in k:\n",
    "        del EXP[k]\n",
    "EXP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fe27ec2-9a77-463f-828a-f541a1e3a511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for name, value in EXP.items():\n",
    "    datasets[name] = Dataset(value)\n",
    "    try:\n",
    "    # if name in ['adult_bias_0.1','adult_bias_0.3']: continue\n",
    "    # if name in ['COMPAS_balanced_0.1']: continue\n",
    "    # if not name in ['income_balanced_0.1', 'income_balanced_0.3']: continue\n",
    "    # if name  not in ['synthetic_20_balanced_0.1', 'synthetic_20_balanced_0.3']: continue\n",
    "        continue\n",
    "    # datasets[name].calculate_probabilities()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3072721e-6dea-4d00-9a91-3d7fe51b0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = list(datasets.keys())\n",
    "configs = [(1, 2), (0.5, 1.5), (0.25, 1.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b681dd0-be59-4527-a89d-c9258286f908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['income_bias_0.1',\n",
       " 'income_bias_0.3',\n",
       " 'income_flip_0.1',\n",
       " 'income_flip_0.3',\n",
       " 'income_balanced_0.1',\n",
       " 'income_balanced_0.3']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = dsets.copy()\n",
    "if dsets[0].split(\"_\")[0] == \"synthetic\":\n",
    "    for i in range(len(d2)):\n",
    "        d2[i] = \"synthetic\" + d2[i][12:]\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c41030e7-19f2-446d-bfbb-ffe0affbb621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking intersections for data leakage...\n",
      "income_bias_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/bias/0.1/\n",
      "Done!\n",
      "income_bias_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/bias/0.3/\n",
      "Done!\n",
      "income_flip_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/flip/0.1/\n",
      "Done!\n",
      "income_flip_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/flip/0.3/\n",
      "Done!\n",
      "income_balanced_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/balanced/0.1/\n",
      "Done!\n",
      "income_balanced_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/1_2/income/balanced/0.3/\n",
      "Done!\n",
      "Checking intersections for data leakage...\n",
      "income_bias_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/bias/0.1/\n",
      "Done!\n",
      "income_bias_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/bias/0.3/\n",
      "Done!\n",
      "income_flip_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/flip/0.1/\n",
      "Done!\n",
      "income_flip_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/flip/0.3/\n",
      "Done!\n",
      "income_balanced_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/balanced/0.1/\n",
      "Done!\n",
      "income_balanced_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.5_1.5/income/balanced/0.3/\n",
      "Done!\n",
      "Checking intersections for data leakage...\n",
      "income_bias_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/bias/0.1/\n",
      "Done!\n",
      "income_bias_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/bias/0.3/\n",
      "Done!\n",
      "income_flip_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/flip/0.1/\n",
      "Done!\n",
      "income_flip_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/flip/0.3/\n",
      "Done!\n",
      "income_balanced_0.1\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/balanced/0.1/\n",
      "Done!\n",
      "income_balanced_0.3\n",
      "data/covariate_shifted_data/semi_synthetic/0.25_1.1/income/balanced/0.3/\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for c in configs:\n",
    "\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    valid_ids = []\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        fold = 'x' + str(i)\n",
    "        exclude = ['prob', 'emp_prob', EXP[dsets[0]]['sensitive_attribute'], EXP[dsets[0]]['label']]\n",
    "        \n",
    "        len_train = len(datasets[dsets[0]].foldwise_data[fold]['train'])\n",
    "        len_test = len(datasets[dsets[0]].foldwise_data[fold]['test'])\n",
    "        len_valid = len(datasets[dsets[0]].foldwise_data[fold]['valid'])\n",
    "        \n",
    "        dataX = datasets[dsets[0]].data\n",
    "        dataX = dataX[list(set(dataX.columns) - set(exclude))]\n",
    "    \n",
    "        tr_idxs, test_idxs = create_shift(dataX, (len_train + len_valid) / (len_train + len_test + len_valid), holdout=0.4, alpha=c[0], beta=c[1])\n",
    "    \n",
    "        omitted = []\n",
    "        for i in range(max(max(tr_idxs), max(test_idxs))):\n",
    "            if i not in tr_idxs and i not in test_idxs:\n",
    "                omitted.append(i)\n",
    "        \n",
    "        tr_data = dataX.iloc[tr_idxs]\n",
    "        test_data = dataX.iloc[test_idxs]\n",
    "        unused_data = dataX.iloc[omitted]\n",
    "        \n",
    "        train_ids.append(tr_idxs[:int(len(tr_idxs) * len_train / (len_train + len_valid))])\n",
    "        test_ids.append(test_idxs)\n",
    "        valid_ids.append(tr_idxs[int(len(tr_idxs) * len_train / (len_train + len_valid)):])\n",
    "\n",
    "    \n",
    "    new_train_ids = pd.DataFrame(train_ids).transpose()\n",
    "    new_train_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "    new_train_ids = new_train_ids.iloc[:-1, :] + 1\n",
    "    \n",
    "    new_test_ids = pd.DataFrame(test_ids).transpose()\n",
    "    new_test_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "    new_test_ids = new_test_ids.iloc[:-1, :] + 1\n",
    "        \n",
    "    new_valid_ids = pd.DataFrame(valid_ids).transpose()\n",
    "    new_valid_ids.columns = ['x' + str(i) for i in range(1, 11)]\n",
    "    new_valid_ids = new_valid_ids.iloc[:-1, :] + 1\n",
    "\n",
    "    # Check if there is data leakage between train and test\n",
    "    print(\"Checking intersections for data leakage...\")\n",
    "    for column in new_test_ids.columns:\n",
    "        if len(list(filter(lambda x: x in new_test_ids[column].tolist(), new_train_ids[column].tolist())) + list(filter(lambda x: x in new_test_ids[column].tolist(), new_valid_ids[column].tolist())) + list(filter(lambda x: x in new_valid_ids[column].tolist(), new_train_ids[column].tolist()))) > 0:\n",
    "            raise Exception(\"ERROR! Intersection between train, test, and valid is not zero!\")\n",
    "    \n",
    "    for d in d2:\n",
    "        print(d)\n",
    "        dset_name = d\n",
    "        splits_path = 'data/covariate_shifted_data/semi_synthetic/' + str(c[0]) + '_' + str(c[1]) + '/' + d.split('_')[0] + '/' + d.split('_')[1] + '/' + d.split('_')[2] + '/'\n",
    "        print(splits_path)\n",
    "        \n",
    "        convert_to_int(new_train_ids).to_csv(splits_path + 'train_ids.csv', index=False)\n",
    "        convert_to_int(new_test_ids).to_csv(splits_path + 'test_ids.csv', index=False)\n",
    "        convert_to_int(new_valid_ids).to_csv(splits_path + 'valid_ids.csv', index=False)\n",
    "        \n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629b598-9e87-4ae9-83dc-4f656aca1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"a\"] + [\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef0cee9f-5816-4543-bbef-b96aa4b1a6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1     1\n",
       "x2     0\n",
       "x3     1\n",
       "x4     0\n",
       "x5     1\n",
       "x6     1\n",
       "x7     0\n",
       "x8     1\n",
       "x9     0\n",
       "x10    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_ids.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b73c361f-fc75-4046-960e-a2bc97e6850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14490.0</td>\n",
       "      <td>34636.0</td>\n",
       "      <td>46133.0</td>\n",
       "      <td>7892.0</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>40450.0</td>\n",
       "      <td>27403.0</td>\n",
       "      <td>2111.0</td>\n",
       "      <td>18180.0</td>\n",
       "      <td>40727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36751.0</td>\n",
       "      <td>11648.0</td>\n",
       "      <td>25231.0</td>\n",
       "      <td>45818.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>28913.0</td>\n",
       "      <td>16572.0</td>\n",
       "      <td>43765.0</td>\n",
       "      <td>11159.0</td>\n",
       "      <td>47609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42807.0</td>\n",
       "      <td>5272.0</td>\n",
       "      <td>5502.0</td>\n",
       "      <td>8485.0</td>\n",
       "      <td>44802.0</td>\n",
       "      <td>25460.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>7953.0</td>\n",
       "      <td>4178.0</td>\n",
       "      <td>38793.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11302.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>34500.0</td>\n",
       "      <td>42161.0</td>\n",
       "      <td>42448.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>19914.0</td>\n",
       "      <td>25368.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6879.0</td>\n",
       "      <td>9458.0</td>\n",
       "      <td>33680.0</td>\n",
       "      <td>14285.0</td>\n",
       "      <td>36016.0</td>\n",
       "      <td>9945.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>28680.0</td>\n",
       "      <td>15119.0</td>\n",
       "      <td>32634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>22719.0</td>\n",
       "      <td>34401.0</td>\n",
       "      <td>14548.0</td>\n",
       "      <td>15828.0</td>\n",
       "      <td>47356.0</td>\n",
       "      <td>30002.0</td>\n",
       "      <td>13501.0</td>\n",
       "      <td>18145.0</td>\n",
       "      <td>17365.0</td>\n",
       "      <td>10114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>19118.0</td>\n",
       "      <td>14974.0</td>\n",
       "      <td>43653.0</td>\n",
       "      <td>20265.0</td>\n",
       "      <td>29725.0</td>\n",
       "      <td>7819.0</td>\n",
       "      <td>24080.0</td>\n",
       "      <td>39502.0</td>\n",
       "      <td>10687.0</td>\n",
       "      <td>35245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>9943.0</td>\n",
       "      <td>20145.0</td>\n",
       "      <td>5483.0</td>\n",
       "      <td>29681.0</td>\n",
       "      <td>29641.0</td>\n",
       "      <td>37720.0</td>\n",
       "      <td>13451.0</td>\n",
       "      <td>9937.0</td>\n",
       "      <td>35328.0</td>\n",
       "      <td>46772.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>42561.0</td>\n",
       "      <td>16755.0</td>\n",
       "      <td>40716.0</td>\n",
       "      <td>16525.0</td>\n",
       "      <td>25566.0</td>\n",
       "      <td>6520.0</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>38984.0</td>\n",
       "      <td>16401.0</td>\n",
       "      <td>44795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23715.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16665.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33429.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2688 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1       x2       x3       x4       x5       x6       x7       x8  \\\n",
       "0     14490.0  34636.0  46133.0   7892.0   3020.0  40450.0  27403.0   2111.0   \n",
       "1     36751.0  11648.0  25231.0  45818.0   1625.0  28913.0  16572.0  43765.0   \n",
       "2     42807.0   5272.0   5502.0   8485.0  44802.0  25460.0   1551.0   7953.0   \n",
       "3     11302.0   5292.0   4721.0  34500.0  42161.0  42448.0   1976.0  19914.0   \n",
       "4      6879.0   9458.0  33680.0  14285.0  36016.0   9945.0   1351.0  28680.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "2683  22719.0  34401.0  14548.0  15828.0  47356.0  30002.0  13501.0  18145.0   \n",
       "2684  19118.0  14974.0  43653.0  20265.0  29725.0   7819.0  24080.0  39502.0   \n",
       "2685   9943.0  20145.0   5483.0  29681.0  29641.0  37720.0  13451.0   9937.0   \n",
       "2686  42561.0  16755.0  40716.0  16525.0  25566.0   6520.0   7142.0  38984.0   \n",
       "2687      NaN  21042.0      NaN  23715.0      NaN      NaN  16665.0      NaN   \n",
       "\n",
       "           x9      x10  \n",
       "0     18180.0  40727.0  \n",
       "1     11159.0  47609.0  \n",
       "2      4178.0  38793.0  \n",
       "3     25368.0    365.0  \n",
       "4     15119.0  32634.0  \n",
       "...       ...      ...  \n",
       "2683  17365.0  10114.0  \n",
       "2684  10687.0  35245.0  \n",
       "2685  35328.0  46772.0  \n",
       "2686  16401.0  44795.0  \n",
       "2687  33429.0      NaN  \n",
       "\n",
       "[2688 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c6bb0c0-e769-4efd-889e-25513041c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1      6\n",
       "x2      2\n",
       "x3      4\n",
       "x4     30\n",
       "x5     14\n",
       "x6      4\n",
       "x7      6\n",
       "x8      1\n",
       "x9      3\n",
       "x10    22\n",
       "dtype: int32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_int(new_test_ids.iloc[:-1, :] + 1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "542dbb35-8d0b-459d-961e-e96e037d98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(df):\n",
    "    \"\"\"Converts all float columns in a dataframe to int, handling casting errors.\n",
    "    \n",
    "    Args:\n",
    "      df: The pandas dataframe to convert.\n",
    "    \n",
    "    Returns:\n",
    "      A new pandas dataframe with all float columns converted to int, \n",
    "      preserving non-numeric columns.\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    for col in df_new.select_dtypes(include=['float']):\n",
    "        try:\n",
    "            # Attempt conversion to int, fill with NaN on errors\n",
    "            df_new[col] = df_new[col].astype(int)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return df_new\n",
    "\n",
    "def dataset_distance(data1, data2):\n",
    "    data = pd.concat([data1, data2], axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    mean1 = np.mean(pc[:len(data1)])\n",
    "    std1 = np.std(pc[:len(data1)])\n",
    "    mean2 = np.mean(pc[len(data1):len(data1)+len(data2)])\n",
    "    std2 = np.std(pc[len(data1):len(data1)+len(data2)])\n",
    "\n",
    "    return mean1, std1, mean2, std2\n",
    "\n",
    "def create_shift(\n",
    "    data,\n",
    "    src_split=0.8,\n",
    "    holdout=0.2,\n",
    "    alpha=1,\n",
    "    beta=2,\n",
    "    kdebw=0.3,\n",
    "    eps=0.001,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates covariate shift sampling of data into disjoint source and target set.\n",
    "\n",
    "    Let \\mu and \\sigma be the mean and the standard deviation of the first principal component retrieved by PCA on the whole data.\n",
    "    The target is randomly sampled based on a Gaussian with mean = \\mu and standard deviation = \\sigma.\n",
    "    The source is randomly sampled based on a Gaussian with mean = \\mu + alpha and standard devaition = \\sigma / beta\n",
    "\n",
    "    data: [m, n]\n",
    "    alpha, beta: the parameter that distorts the gaussian used in sampling\n",
    "                   according to the first principle component\n",
    "    output: source indices, target indices, ratios based on kernel density estimation with bandwidth = kdebw and smoothed by eps\n",
    "    \"\"\"\n",
    "    m = np.shape(data)[0]\n",
    "    source_size = int(m * src_split * (1 - holdout))\n",
    "    target_size = int(m * (1 - src_split) * (1 - holdout))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pc2 = pca.fit_transform(data)\n",
    "    pc = pc2[:, 0]\n",
    "    pc = pc.reshape(-1, 1)\n",
    "\n",
    "    pc_mean = np.mean(pc)\n",
    "    pc_std = np.std(pc)\n",
    "\n",
    "    sample_mean = pc_mean + alpha\n",
    "    sample_std = pc_std / beta\n",
    "\n",
    "    # sample according to the probs\n",
    "    prob_s = norm.pdf(pc, loc=sample_mean, scale=sample_std)\n",
    "    sum_s = np.sum(prob_s)\n",
    "    prob_s = prob_s / sum_s\n",
    "    prob_t = norm.pdf(pc, loc=pc_mean, scale=pc_std)\n",
    "    sum_t = np.sum(prob_t)\n",
    "    prob_t = prob_t / sum_t\n",
    "\n",
    "    source_ind = np.random.choice(\n",
    "        range(m), size=source_size, replace=False, p=np.reshape(prob_s, (m))\n",
    "    )\n",
    "\n",
    "    pt_proxy = np.copy(prob_t)\n",
    "    pt_proxy[source_ind] = 0\n",
    "    pt_proxy = pt_proxy / np.sum(pt_proxy)\n",
    "    target_ind = np.random.choice(\n",
    "        range(m), size=target_size, replace=False, p=np.reshape(pt_proxy, (m))\n",
    "    )\n",
    "\n",
    "    return source_ind, target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4b288-f257-474c-843a-e6d1f1913237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
