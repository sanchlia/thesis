# Copyright (c) Microsoft Corporation and Fairlearn contributors.
# Licensed under the MIT License.

import numpy as np
import pandas as pd

from fairlearn.utils._input_validation import _validate_and_reformat_input

from .moment import _ALL, _LABEL, ClassificationMoment

_MESSAGE_BAD_COSTS = (
    "costs needs to be a dictionary with keys "
    "'fp' and 'fn' containing non-negative values, which are not both zero"
)


class ErrorRate(ClassificationMoment):
    r"""Misclassification error as a moment.

    A classifier :math:`h(X)` has the misclassification error equal to

    .. math::
      P[h(X) \ne Y]

    It is also possible to specify costs for false positives and false
    negatives. The error then evaluates to

    .. math::
      c_{FP} P[h(X)=1, Y=0] + c_{FN} P[h(X)=0, Y=1]

    where :math:`c_{FP}` and :math:`c_{FN}` are the costs of false positive
    and false negative errors respectively. The standard misclassification
    error corresponds to :math:`c_{FP}=c_{FN}=1.0`.

    Read more in the :ref:`User Guide <error_rate_parity>`.

    Parameters
    ----------
    costs : dict
        The dictionary with keys :code:`'fp'` and :code:`'fn'` containing the
        costs of false positives and false negatives. If none are provided
        costs of 1.0 are assumed.
    """

    short_name = "Err"

    def __init__(self, *, tags = None, costs=None):
        """Initialize the costs."""
        super(ErrorRate, self).__init__()
        if costs is None:
            self.fp_cost = 1.0
            self.fn_cost = 1.0
        elif (
            type(costs) is dict
            and costs.keys() == {"fp", "fn"}
            and costs["fp"] >= 0.0
            and costs["fn"] >= 0.0
            and costs["fp"] + costs["fn"] > 0.0
        ):
            self.fp_cost = costs["fp"]
            self.fn_cost = costs["fn"]
        else:
            raise ValueError(_MESSAGE_BAD_COSTS)

    def load_data(self, X, y, *, sensitive_features, control_features=None):
        """Load the specified data into the object."""

        _, y_train, sf_train, _ = _validate_and_reformat_input(
            X,
            y,
            enforce_binary_labels=True,
            sensitive_features=sensitive_features,
            control_features=control_features,
        )


        # The following uses X  so that the estimators get X untouched
        super().load_data(X.values, y_train, sensitive_features=sf_train)
        self.index = [_ALL]

    def gamma(self, predictor, X = None, tags = None, weights = None):
        """Return the gamma values for the given predictor."""
        # print(f'in error rate gamma {self.X.shape=}')
        if tags is None and X is None:
            X = self.X
            tags = self.tags
        # print(f'updates x is {X.shape=}')
        pred = predictor(X)
        if isinstance(pred, np.ndarray):
            # TensorFlow is returning an (n,1) array, which results
            # in the subtraction in the 'error =' line generating an
            # (n,n) array
            pred = np.squeeze(pred)
        

        # print(f'{tags[_LABEL].value_counts()=}')
        # print(f'{pred.value_counts()}')
        signed_errors = tags[_LABEL] - pred
        try:
            # total_fn_cost = np.sum(-signed_errors[signed_errors < 0] * (1 - tags.loc[signed_errors < 0]["weights"]))
            # total_fp_cost = np.sum(signed_errors[signed_errors > 0] * (1 - tags.loc[signed_errors > 0]["weights"]))
            # fp_cost = np.where(self.tags[_LABEL] == 0, self.tags["weights"], 0)
            # fn_cost = np.where(self.tags[_LABEL] == 1, self.tags["weights"], 0)
            # total_fn_cost = np.sum(signed_errors[signed_errors > 0] * fn_cost)
            # total_fp_cost = np.sum(-signed_errors[signed_errors < 0] * fp_cost)

            total_fn_cost = np.sum(signed_errors[signed_errors > 0] * tags.loc[signed_errors > 0]["weights"])
            total_fp_cost = np.sum(-signed_errors[signed_errors < 0] * tags.loc[signed_errors < 0]["weights"])
        except KeyError as ke:
            # print(f'is there key error ? {ke=}')
            total_fn_cost = np.sum(signed_errors[signed_errors > 0] * self.fn_cost)
            total_fp_cost = np.sum(-signed_errors[signed_errors < 0] * self.fp_cost)
        print(f'{total_fn_cost=} and {total_fp_cost=}')
        error_value = (total_fn_cost + total_fp_cost) / tags.shape[0]
        # error_value = (total_fn_cost + total_fp_cost) / self.total_samples

        error = pd.Series(data=error_value, index=self.index)
        # print(f'In gamma, {error=}')
        self._gamma_descr = str(error)
        return error

    def project_lambda(self, lambda_vec):
        """Return the lambda values."""
        return lambda_vec

    def signed_weights(self, lambda_vec=None, tags = None, fair_df = None):
        """Return the signed weights."""
        # print("error rate: signed weights called")

        if fair_df is not None:
            # FP: if y_hat == 1, P(Y_f = 0 | X,S, Y_o = yi)
            # Y_o_i -> Wi 
            # transform FP cost and FN cost for the entire set 
            #
            self.fp_cost = np.where(fair_df[_LABEL] == 0, fair_df["weights"], 0)
            self.fn_cost = np.where(fair_df[_LABEL] == 1, fair_df["weights"], 0)
            weights = -self.fp_cost + (self.fp_cost + self.fn_cost) * tags[_LABEL]
            # combined_ = pd.concat((fn_cost, fp_cost))
            # weights = combined_ * self.tags[_LABEL]
            # weights = -prob_event[1] + (prob_event[1] + prob_event[0]) * self.tags[_LABEL]
        else:
            weights = -self.fp_cost + (self.fp_cost + self.fn_cost) * self.tags[_LABEL]
        if lambda_vec is None:
            return weights
        else:
            return lambda_vec[_ALL] * weights
