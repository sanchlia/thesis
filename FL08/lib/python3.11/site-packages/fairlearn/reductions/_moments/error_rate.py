# Copyright (c) Microsoft Corporation and Fairlearn contributors.
# Licensed under the MIT License.

import numpy as np
import pandas as pd

from fairlearn.utils._input_validation import _validate_and_reformat_input

from .moment import _ALL, _LABEL, ClassificationMoment

_MESSAGE_BAD_COSTS = (
    "costs needs to be a dictionary with keys "
    "'fp' and 'fn' containing non-negative values, which are not both zero"
)


class ErrorRate(ClassificationMoment):
    r"""Misclassification error as a moment.

    A classifier :math:`h(X)` has the misclassification error equal to

    .. math::
      P[h(X) \ne Y]

    It is also possible to specify costs for false positives and false
    negatives. The error then evaluates to

    .. math::
      c_{FP} P[h(X)=1, Y=0] + c_{FN} P[h(X)=0, Y=1]

    where :math:`c_{FP}` and :math:`c_{FN}` are the costs of false positive
    and false negative errors respectively. The standard misclassification
    error corresponds to :math:`c_{FP}=c_{FN}=1.0`.

    Read more in the :ref:`User Guide <error_rate_parity>`.

    Parameters
    ----------
    costs : dict
        The dictionary with keys :code:`'fp'` and :code:`'fn'` containing the
        costs of false positives and false negatives. If none are provided
        costs of 1.0 are assumed.
    """

    short_name = "Err"

    def __init__(self,  *, costs=None):
        """Initialize the costs."""
        super(ErrorRate, self).__init__()
        if costs is None:
            self.fp_cost = 1.0
            self.fn_cost = 1.0
        elif (
            type(costs) is dict
            and costs.keys() == {"fp", "fn"}
            and (costs["fp"] >= 0.0).all()
            and (costs["fn"] >= 0.0).all()
            and (costs["fp"] + costs["fn"] > 0.0).all()
        ):
            self.fp_cost = costs["fp"]
            self.fn_cost = costs["fn"]
        else:
            raise ValueError(_MESSAGE_BAD_COSTS)
        
    # def _transform(self, X, y, sensitive_features, fair_weights):
    #     positive_df, negative_df = pd.DataFrame({"label": y, "sensitive_features": sensitive_features}), pd.DataFrame({"label": y, "sensitive_features": sensitive_features})
    #     positive_df['Yf'] = 1
    #     negative_df['Yf'] = 0
    #     positive_df['weights'] = fair_weights
    #     negative_df['weights'] = 1- fair_weights
    #     if not self.validate_probs(positive_df["weights"]) or not self.validate_probs(negative_df['weights']):
    #         print('Error')
    #         raise Exception
        
    #     _y = pd.concat([y,y]).reset_index(drop= True)
    #     # _y = pd.concat([positive_df['Yf'],negative_df['Yf']]).reset_index(drop= True)
    #     _sensitive_features = pd.concat([sensitive_features, sensitive_features]).reset_index(drop= True)
    #     _X = pd.concat([X,X]).reset_index(drop= True)
    #     _fair_weights = pd.concat([positive_df['weights'], negative_df['weights']]).reset_index(drop= True)
    #     if not self._validate(_X, _y, _sensitive_features, _fair_weights):
    #         print('Error in validation')
    #         raise Exception
    #     # return _X, _y, _sensitive_features, _fair_weights
    #     return  _fair_weights

    def _validate(self,X, y, sensitive_features, fair_weights):
        n = len(X)
        for i in range(n//2):
            if ((fair_weights.iloc[i] + fair_weights[i + n//2] == 1) and (X.iloc[i] == X.iloc[i + n//2]).all() and (sensitive_features.iloc[i] == sensitive_features.iloc[i + n//2])): 
                continue
            else:
                return False
        return True
    def validate_probs(self,prob: pd.Series):
        if (prob > 1).any():
            return False
        return True
    
    def load_data(self, X, y, *, fair_weights, sensitive_features, control_features=None):
        """Load the specified data into the object."""
        if fair_weights is not None:
            # X, y, sensitive_features, _fair_weights  = self._transform(X, y, sensitive_features, fair_weights['weights'])
            # _fair_weights  = self._transform(X, y, sensitive_features, fair_weights['weights'])
            self._fair_weights = fair_weights['weights']
        _, y_train, sf_train, _ = _validate_and_reformat_input(
            X,
            y,
            enforce_binary_labels=True,
            sensitive_features=sensitive_features,
            control_features=control_features,
        )
        # The following uses X  so that the estimators get X untouched
        super().load_data(X, y_train, sensitive_features=sf_train, fair_weights= self._fair_weights)
        self.index = [_ALL]
    

    
    def gamma(self, predictor):
        """Return the gamma values for the given predictor."""
        pred = predictor(self.X)
        if isinstance(pred, np.ndarray):
            # TensorFlow is returning an (n,1) array, which results
            # in the subtraction in the 'error =' line generating an
            # (n,n) array
            pred = np.squeeze(pred)
        signed_errors = self.tags[_LABEL] - pred
        # total_fn_cost = np.sum(signed_errors[signed_errors > 0] * self.fn_cost)
        # total_fp_cost = np.sum(-signed_errors[signed_errors < 0] * self.fp_cost)

        total_fn_cost = np.sum(signed_errors[signed_errors > 0] * self.fn_cost[signed_errors>0])
        total_fp_cost = np.sum(-signed_errors[signed_errors < 0] * self.fp_cost[signed_errors<0])
        error_value = (total_fn_cost + total_fp_cost) / self.total_samples
        error = pd.Series(data=error_value, index=self.index)
        self._gamma_descr = str(error)
        return error

    def project_lambda(self, lambda_vec):
        """Return the lambda values."""
        return lambda_vec

    def signed_weights(self, lambda_vec=None):
        """Return the signed weights."""
        weights = -self.fp_cost + (self.fp_cost + self.fn_cost) * self.tags[_LABEL]
        if lambda_vec is None:
            return weights
        else:
            return lambda_vec[_ALL] * weights
