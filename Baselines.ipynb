{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07af936f-313f-471d-8d84-57ecf964ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from math import isclose, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from KDEpy import NaiveKDE\n",
    "\n",
    "from bin.dataset import Dataset\n",
    "from bin.experiment import Experiment\n",
    "from bin.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f00d169-0dea-436f-ab6b-ae65d411f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    \"\"\"Reads the config file and returns a dictionary.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found.\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads the csv file and returns a dataframe.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found.\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f17580a1-4e22-4e15-9b73-867520ef5e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['COMPAS_bias_0.1', 'COMPAS_bias_0.3', 'COMPAS_flip_0.1', 'COMPAS_flip_0.3', 'COMPAS_balanced_0.1', 'COMPAS_balanced_0.3', 'COMPAS_bias_0.1_cov', 'COMPAS_bias_0.3_cov', 'COMPAS_flip_0.1_cov', 'COMPAS_flip_0.3_cov', 'COMPAS_balanced_0.1_cov', 'COMPAS_balanced_0.3_cov'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_conf = \"configs/COMPAS_noisy.json\"\n",
    "EXP = read_config(exp_conf)\n",
    "EXP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "330169e5-59c2-4786-a042-75a939de41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "try:\n",
    "    for name, value in EXP.items():\n",
    "        # if name in ['adult_bias_0.1','adult_bias_0.3']: continue\n",
    "        # if name in ['COMPAS_balanced_0.1']: continue\n",
    "        if not name in ['COMPAS_bias_0.1', 'COMPAS_bias_0.3']: continue\n",
    "        # if name  not in ['synthetic_20_balanced_0.1', 'synthetic_20_balanced_0.3']: continue\n",
    "        datasets[name] = Dataset(value)\n",
    "        # datasets[name].calculate_probabilities()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f006888-56b6-43d9-9c7d-6e5588a891eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPAS_bias_0.1': <bin.dataset.Dataset at 0x7fa64f5c25d0>,\n",
       " 'COMPAS_bias_0.3': <bin.dataset.Dataset at 0x7fa6500123d0>}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_dataset = 'COMPAS_bias_0.1'\n",
    "d = datasets[chosen_dataset]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "543b67ef-fb95-4393-b199-670176cb91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'x1'\n",
    "\n",
    "train_data = d.foldwise_data[fold]['train']\n",
    "test_data = d.foldwise_data[fold]['test']\n",
    "\n",
    "# X label, Y label, and sensitive attribute\n",
    "X_src = train_data.drop([EXP[chosen_dataset]['label'], 'prob', 'emp_prob'], axis=1).to_numpy()\n",
    "Y_src = train_data.drop(train_data.columns.difference([EXP[chosen_dataset]['label']]), axis=1).to_numpy()[:, 0]\n",
    "S_src = train_data.drop(train_data.columns.difference([EXP[chosen_dataset]['sensitive_attribute']]), axis=1).to_numpy()[:, 0]\n",
    "X_trg = test_data.drop([EXP[chosen_dataset]['label'], 'prob', 'emp_prob'], axis=1).to_numpy()\n",
    "Y_trg = test_data.drop(test_data.columns.difference([EXP[chosen_dataset]['label']]), axis=1).to_numpy()[:, 0]\n",
    "S_trg = test_data.drop(test_data.columns.difference([EXP[chosen_dataset]['sensitive_attribute']]), axis=1).to_numpy()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d0edde8-5616-4118-924b-e4b51f327522",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = eopp_fair_covariate_shift_logloss(\n",
    "        verbose=1, tol=1e-7, random_initialization=False, trg_grg_marginal_matching_covariate_shift=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dfab65c3-4513-4332-b42d-c9e334b906e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 : 0.5941, p0 : 0.2892\n",
      "Mu = -1.500 - Epoch 2: converged, error\t0.38, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu = 1.500 - Epoch 3: converged, error\t0.13, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Grid search: mu range(-1.50,-1.00)\n",
      "Mu = -1.000 - Epoch 3: converged, error\t0.38, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Grid search: mu range(-1.00,-0.50)\n",
      "Mu = -0.500 - Epoch 2: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Grid search: mu range(-0.50,0.00)\n",
      "Mu = 0.000 - Epoch 2: converged, error\t0.12, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Found zero point range.\n",
      "Mu = -0.500 - Epoch 2: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu = 0.000 - Epoch 2: converged, error\t0.12, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Binary search for zero violation in Mu ranges: [[0.0, -0.5]]\n",
      "Mu = -0.250 - Epoch 2: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu range: [0.000, -0.500] => c = -0.2500, q-violation = -0.0158\n",
      "Mu = -0.375 - Epoch 2: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu range: [-0.250, -0.500] => c = -0.3750, q-violation = -0.0079\n",
      "Mu = -0.438 - Epoch 1: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu range: [-0.375, -0.500] => c = -0.4375, q-violation = -0.0034\n",
      "Mu = -0.469 - Epoch 1: converged, error\t0.11, |grad_theta|:\t0.0000001, |grad_lambda|:\t0.0000000\n",
      "Mu range: [-0.438, -0.500] => c = -0.4688, q-violation = -0.0008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.eopp_fair_covariate_shift_logloss at 0x7fa5ff4186d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.trg_grp_marginal_matching = True\n",
    "h.C = 0.001\n",
    "h.max_epoch = 3\n",
    "h.max_iter = 3000\n",
    "h.tol = 1e-7\n",
    "h.random_start = True\n",
    "h.verbose = 1\n",
    "h.fit(\n",
    "    X_src,\n",
    "    Y_src,\n",
    "    S_src,\n",
    "    np.ones_like(S_src), # Not providing ratio\n",
    "    X_trg,\n",
    "    S_trg,\n",
    "    np.ones_like(S_trg), # Not providing ratio\n",
    "    mu_range=[-1.5,1.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b2198ff-a18c-4cb7-a63a-1be9e45d4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - prediction_err : 0.114\t fairness_violation : 0.017 \n"
     ]
    }
   ],
   "source": [
    "err = 1 - h.score(\n",
    "    X_trg, Y_trg, S_trg, np.ones_like(S_trg)\n",
    "    )\n",
    "\n",
    "violation = abs(\n",
    "    h.fairness_violation(\n",
    "        X_trg, Y_trg, S_trg, np.ones_like(S_trg)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\n",
    "        \"Test  - prediction_err : {:.3f}\\t fairness_violation : {:.3f} \".format(\n",
    "            err, violation\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5d95c-6c20-4714-a11a-913f2f716177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdb7df-198d-4e21-83e4-324c559f5fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671dd0cd-b892-4441-bf27-55a30ea473d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124580dd-c113-44ac-b53d-0b39d4caa754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dot_intercept(w, X):\n",
    "    c = 0\n",
    "    if w.size == X.shape[1] + 1:\n",
    "        c = w[-1]\n",
    "        w = w[:-1]\n",
    "\n",
    "    z = np.dot(X, w) + c\n",
    "    return z\n",
    "\n",
    "\n",
    "class eopp_fair_covariate_shift_logloss:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tol=1e-6,\n",
    "        verbose=0,\n",
    "        max_iter=10000,\n",
    "        C=0.001,\n",
    "        random_initialization=True,\n",
    "        trg_grp_marginal_matching=True,\n",
    "        trg_grg_marginal_matching_covariate_shift=False,\n",
    "    ):\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.random_start = random_initialization\n",
    "        self.theta = None\n",
    "        self.lb_prob = 1e-9\n",
    "        self.trg_grp_marginal_matching = trg_grp_marginal_matching\n",
    "        self.max_epoch = 5\n",
    "        self.trg_group_estimator = None\n",
    "        self.trg_grp_marginal_matching_covariate_shift = (\n",
    "            trg_grg_marginal_matching_covariate_shift\n",
    "        )\n",
    "        self.lambdas = np.zeros((2,))\n",
    "\n",
    "    def f_value(self, A):\n",
    "        if self.trg_group_estimator(1) >= self.trg_group_estimator(0):\n",
    "            f_val = np.where(\n",
    "                A == 1,\n",
    "                1,\n",
    "                -self.trg_group_estimator(1) / self.trg_group_estimator(0),\n",
    "            )\n",
    "        else:\n",
    "            f_val = np.where(\n",
    "                A == 1,\n",
    "                self.trg_group_estimator(0) / self.trg_group_estimator(1),\n",
    "                -1,\n",
    "            )\n",
    "        return f_val\n",
    "\n",
    "    def build_trg_grp_estimator(\n",
    "        self, X_src, A_src, Y_src, src_ratio, X_trg, A_trg, trg_ratio\n",
    "    ):\n",
    "        h = eopp_fair_covariate_shift_logloss(\n",
    "            tol=self.tol,\n",
    "            max_iter=self.max_iter,\n",
    "            C=self.C,\n",
    "            random_initialization=False,\n",
    "            verbose=False,\n",
    "            trg_grp_marginal_matching=False,\n",
    "        )\n",
    "        h.trg_group_estimator = lambda a: 1  # dummy estimator, not used because mu = 0\n",
    "        h.fit(\n",
    "            X_src, Y_src, A_src, src_ratio, X_trg, A_trg, trg_ratio, mu_range=0\n",
    "        )  # build a covaritae shift model with ignored fairness\n",
    "        p = h.predict_proba(\n",
    "            X_trg, A_trg, trg_ratio\n",
    "        )  # A is ignored as attribute but is included in X\n",
    "        p1 = np.dot(p, A_trg.astype(\"int\")) / A_trg.shape[0]\n",
    "        p0 = np.dot(p, 1 - A_trg.astype(\"int\")) / A_trg.shape[0]\n",
    "        if self.verbose >= 1:\n",
    "            print(\"p1 : {:.4f}, p0 : {:.4f}\".format(p1, p0))\n",
    "        estimator = lambda a: p1 if (a == 1) else p0 if (a == 0) else 0\n",
    "        return estimator\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_src,\n",
    "        Y_src,\n",
    "        A_src,\n",
    "        src_st_ratio,\n",
    "        X_trg,\n",
    "        A_trg,\n",
    "        trg_st_ratio,\n",
    "        mu_range=[-1, 1],\n",
    "    ):\n",
    "\n",
    "        if not self.trg_group_estimator:\n",
    "            self.trg_group_estimator = self.build_trg_grp_estimator(\n",
    "                X_src,\n",
    "                A_src,\n",
    "                Y_src,\n",
    "                (\n",
    "                    src_st_ratio\n",
    "                    if self.trg_grp_marginal_matching_covariate_shift\n",
    "                    else np.ones_like(src_st_ratio)\n",
    "                ),\n",
    "                X_trg,\n",
    "                A_trg,\n",
    "                (\n",
    "                    trg_st_ratio\n",
    "                    if self.trg_grp_marginal_matching_covariate_shift\n",
    "                    else np.ones_like(trg_st_ratio)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        def _fit_given_mu(\n",
    "            X_src, Y_src, A_src, src_st_ratio, X_trg, A_trg, trg_st_ratio, mu\n",
    "        ):\n",
    "            X_src = np.hstack((X_src, np.ones((X_src.shape[0], 1))))\n",
    "            X_trg = np.hstack((X_trg, np.ones((X_trg.shape[0], 1))))\n",
    "\n",
    "            def init_theta(mu, X_src, Y_src, A_src, src_st_ratio):\n",
    "                m = X_src.shape[1]\n",
    "                theta = self.theta\n",
    "                if theta is None:\n",
    "                    if self.random_start:\n",
    "                        theta = np.random.random_sample((m,)) - 0.5\n",
    "                    else:\n",
    "                        theta = np.zeros((m,))\n",
    "                res = minimize(\n",
    "                    compute_theta_loss_grad,\n",
    "                    theta,\n",
    "                    args=(self.lambdas, mu, X_src, Y_src, A_src, src_st_ratio),\n",
    "                    method=\"L-BFGS-B\",\n",
    "                    jac=True,\n",
    "                    tol=1e-12,\n",
    "                    options={\n",
    "                        \"maxiter\": self.max_iter,\n",
    "                        \"disp\": self.verbose >= 3,\n",
    "                        \"gtol\": self.tol,\n",
    "                    },\n",
    "                )\n",
    "                theta = res.x\n",
    "                return theta\n",
    "\n",
    "            \"\"\"\n",
    "            This function computes loss and grad for passing to optimization function.\n",
    "            Lambdas are concated to the returned grad\n",
    "            \"\"\"\n",
    "\n",
    "            def compute_theta_loss_grad(\n",
    "                theta, lambdas, mu, X_src, Y_src, A_src, src_st_ratio\n",
    "            ):\n",
    "                # The gradient is computed on src data\n",
    "                p, q = self._compute_p_and_q(\n",
    "                    theta, lambdas, mu, X_src, A_src, src_st_ratio\n",
    "                )\n",
    "                n = X_src.shape[0]\n",
    "                z = _dot_intercept(theta, X_src)\n",
    "                loss = 1 / src_st_ratio * (\n",
    "                    -q * np.log(p)\n",
    "                    - (1 - q) * np.log(1 - p)\n",
    "                    + mu * p * q * self.f_value(A_src)\n",
    "                ) + z * (q - Y_src)\n",
    "                loss = np.mean(loss) + 0.5 * self.C * np.dot(theta, theta)\n",
    "\n",
    "                grad = np.reshape(q - Y_src, (-1, 1)) * X_src  # todo no ratio\n",
    "\n",
    "                grad = np.sum(grad, axis=0) / n + self.C * theta\n",
    "\n",
    "                return loss, grad\n",
    "\n",
    "            def compute_lambda_loss_grad(lambdas, theta, mu, X_trg, A_trg, st_ratio):\n",
    "                if (\n",
    "                    not self.trg_grp_marginal_matching\n",
    "                ):  # or self.mu == 0: # inactive fairness\n",
    "                    return 0, 0\n",
    "                C = 0\n",
    "                p, q = self._compute_p_and_q(theta, lambdas, mu, X_trg, A_trg, st_ratio)\n",
    "                n = q.shape[0]\n",
    "                g1 = np.dot(q, A_trg) / n - self.trg_group_estimator(1)\n",
    "                g0 = np.dot(q, 1 - A_trg) / n - self.trg_group_estimator(0)\n",
    "                loss = (\n",
    "                    g1 * lambdas[0]\n",
    "                    + g0 * lambdas[1]\n",
    "                    + 0.5 * C * np.dot(lambdas, lambdas)\n",
    "                )\n",
    "                grad = np.array([g1, g0]).reshape((-1,))\n",
    "                grad = grad + C * lambdas\n",
    "                return loss, grad\n",
    "\n",
    "            def find_theta_lambda_with_grad(\n",
    "                mu, X_src, Y_src, A_src, src_st_ratio, X_trg, A_trg, trg_st_ratio\n",
    "            ):\n",
    "                lambdas = self.lambdas\n",
    "                theta = self.theta\n",
    "                lambda_rate = 1  # learning rate\n",
    "                theta_rate = 0.01\n",
    "                max_itr = self.max_iter\n",
    "                min_val_t = self.tol\n",
    "                min_val_l = 1e-6\n",
    "\n",
    "                S_g_t = np.ones_like(theta) * 1e-8  # prevent dividing by zero\n",
    "                S_g_l = np.ones_like(lambdas) * 1e-8  # prevent dividing by zero\n",
    "                l_0 = 0\n",
    "                l_1 = (1 + sqrt(1 + 4 * l_0 ** 2)) / 2\n",
    "                delta_1_l, delta_1_t = 0, 0\n",
    "                t = 1\n",
    "                while True:\n",
    "                    t = t + 1\n",
    "                    decay = sqrt(1000 / (1000 + t))\n",
    "                    l_2 = (1 + sqrt(1 + 4 * l_1 ** 2)) / 2\n",
    "                    l_3 = (1 - l_1) / l_2\n",
    "                    _, G_l = compute_lambda_loss_grad(\n",
    "                        lambdas, theta, mu, X_trg, A_trg, trg_st_ratio\n",
    "                    )\n",
    "                    _, G_t = compute_theta_loss_grad(\n",
    "                        theta, lambdas, mu, X_src, Y_src, A_src, src_st_ratio\n",
    "                    )\n",
    "                    if self.verbose >= 2:\n",
    "                        if t % 1000 == 0:\n",
    "                            print(\n",
    "                                \"Lambda gnorm {:.7f}, Theta norm {:.7f}\".format(\n",
    "                                    np.linalg.norm(G_l), np.linalg.norm(G_t)\n",
    "                                )\n",
    "                            )\n",
    "                    if (\n",
    "                        np.linalg.norm(G_t) < min_val_t\n",
    "                        and np.linalg.norm(G_l) < min_val_l\n",
    "                    ):  # convergence threshold\n",
    "                        if self.verbose >= 2:\n",
    "                            print(\n",
    "                                \"-> GD epoch: converged. |grad_theta|:\\t{:.9f}, |grad_lambda|:\\t{:.9f}\".format(\n",
    "                                    np.linalg.norm(G_t), np.linalg.norm(G_l)\n",
    "                                )\n",
    "                            )\n",
    "                        break\n",
    "                    elif t > max_itr:\n",
    "                        if self.verbose >= 2:\n",
    "                            print(\n",
    "                                \"-> GD epoch: max iteration stopped. |grad_theta|:\\t{:.9f}, |grad_lambda|:\\t{:.9f}\".format(\n",
    "                                    np.linalg.norm(G_t), np.linalg.norm(G_l)\n",
    "                                )\n",
    "                            )\n",
    "                        break\n",
    "                    S_g_t = S_g_t + np.square(G_t)  # for adaptive gradient\n",
    "                    S_g_l = S_g_l + np.square(G_l)  # for adaptive gradient\n",
    "                    delta_2_l = lambdas - decay * lambda_rate * G_l / np.sqrt(\n",
    "                        S_g_l\n",
    "                    )  # adaptive gradient and Nesterov's Accelerated Gradient Descent\n",
    "                    delta_2_t = theta - decay * theta_rate * G_t / np.sqrt(\n",
    "                        S_g_t\n",
    "                    )  # adaptive gradient and Nesterov's Accelerated Gradient Descent\n",
    "                    lambdas = (1 - l_3) * delta_2_l + l_3 * delta_1_l\n",
    "                    theta = (1 - l_3) * delta_2_t + l_3 * delta_1_t\n",
    "                    delta_1_l = delta_2_l\n",
    "                    delta_1_t = delta_2_t\n",
    "                    l_1 = l_2\n",
    "                return theta, lambdas, G_l, G_t\n",
    "\n",
    "            self.theta = init_theta(mu, X_src, Y_src, A_src, src_st_ratio)\n",
    "\n",
    "            epoch = 1\n",
    "            while True:\n",
    "                self.theta, self.lambdas, l_g, t_g = find_theta_lambda_with_grad(\n",
    "                    mu, X_src, Y_src, A_src, src_st_ratio, X_trg, A_trg, trg_st_ratio\n",
    "                )\n",
    "                if np.linalg.norm(l_g) < self.tol and np.linalg.norm(t_g) < self.tol:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(\n",
    "                            \"Mu = {:.3f} - Epoch {:d}: converged, error\\t{:.2f}, |grad_theta|:\\t{:.7f}, |grad_lambda|:\\t{:.7f}\".format(\n",
    "                                mu,\n",
    "                                epoch,\n",
    "                                1\n",
    "                                - self._score(\n",
    "                                    X_src,\n",
    "                                    Y_src,\n",
    "                                    A_src,\n",
    "                                    src_st_ratio,\n",
    "                                    mu,\n",
    "                                    self.lambdas,\n",
    "                                    self.theta,\n",
    "                                ),\n",
    "                                np.linalg.norm(t_g),\n",
    "                                np.linalg.norm(l_g),\n",
    "                            )\n",
    "                        )\n",
    "                    break\n",
    "                elif epoch >= self.max_epoch:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(\n",
    "                            \"Mu = {:.3f} - Epoch {:d} NOT converged, error\\t{:.2f}, |grad_theta|:\\t{:.7f}, |grad_lambda|\\t{:.7f}\".format(\n",
    "                                mu,\n",
    "                                epoch,\n",
    "                                1\n",
    "                                - self._score(\n",
    "                                    X_src,\n",
    "                                    Y_src,\n",
    "                                    A_src,\n",
    "                                    src_st_ratio,\n",
    "                                    mu,\n",
    "                                    self.lambdas,\n",
    "                                    self.theta,\n",
    "                                ),\n",
    "                                np.linalg.norm(t_g),\n",
    "                                np.linalg.norm(l_g),\n",
    "                            )\n",
    "                        )\n",
    "                    break\n",
    "                epoch += 1\n",
    "            return self.theta, self.lambdas\n",
    "\n",
    "        def q_violation_given_mu(mu):\n",
    "            _fit_given_mu(\n",
    "                X_src, Y_src, A_src, src_st_ratio, X_trg, A_trg, trg_st_ratio, mu\n",
    "            )\n",
    "            return self.q_fairness_violation(X_trg, A_trg, trg_st_ratio, mu)\n",
    "\n",
    "        def _binary_search_mu(mu0, mu1):\n",
    "            a, b = mu0, mu1\n",
    "            fa = q_violation_given_mu(a)\n",
    "            fb = q_violation_given_mu(b)\n",
    "            if fa > fb:\n",
    "                a, b = b, a\n",
    "                fa, fb = fb, fa\n",
    "            if isclose(fa, 0, abs_tol=1e-4):\n",
    "                return a\n",
    "            elif isclose(fb, 0, abs_tol=1e-4):\n",
    "                return b\n",
    "            elif fa * fb > 0:\n",
    "                print(\n",
    "                    \"mu: [{:.3f}, {:.3f}],f(mu): [{:.4f}, {:.4f}]\".format(a, b, fa, fb)\n",
    "                )\n",
    "                raise ValueError(\n",
    "                    \"Mu range boundary is both positive or negative. Try a wider range!.\"\n",
    "                )\n",
    "            else:\n",
    "                self.zero_regions = [[a, b]]\n",
    "                print(\n",
    "                    \"Binary search for zero violation in Mu ranges:\", self.zero_regions\n",
    "                )\n",
    "                for r in self.zero_regions:\n",
    "                    a, b = r[0], r[1]\n",
    "                    while not isclose(a - b, 0, abs_tol=1e-4):\n",
    "                        c = (a + b) / 2\n",
    "                        fc = q_violation_given_mu(c)\n",
    "                        print(\n",
    "                            \"Mu range: [{:.3f}, {:.3f}] => c = {:.4f}, q-violation = {:.4f}\".format(\n",
    "                                a, b, c, fc\n",
    "                            )\n",
    "                        )\n",
    "                        if isclose(abs(fc), 0, abs_tol=1e-3):\n",
    "                            return c\n",
    "                        elif fc < 0:\n",
    "                            a = c\n",
    "                        elif fc > 0:\n",
    "                            b = c\n",
    "\n",
    "        # end _binary_search_mu()\n",
    "\n",
    "        def find_valid_mu_range(mu_range, step=0.5):\n",
    "            if np.isscalar(mu_range):\n",
    "                return mu_range\n",
    "            if len(mu_range) < 2:\n",
    "                return mu_range[0]\n",
    "\n",
    "            mu0 = mu_range[0]\n",
    "            mu1 = mu_range[1]\n",
    "            if mu1 < mu0:\n",
    "                mu0, mu1 = mu1, mu0\n",
    "\n",
    "            lo = q_violation_given_mu(mu0)\n",
    "            hi = q_violation_given_mu(mu1)\n",
    "\n",
    "            mu_lo = mu0\n",
    "            for mu_ in np.arange(mu0 + step, mu1 + step, step):\n",
    "                if self.verbose >= 1:\n",
    "                    print(\"Grid search: mu range({:.2f},{:.2f})\".format(mu_lo, mu_))\n",
    "                v_ = q_violation_given_mu(mu_)\n",
    "                if v_ * lo < 0:\n",
    "                    if self.verbose >= 1:\n",
    "                        print(\"Found zero point range.\")\n",
    "                    return [mu_lo, mu_]\n",
    "                else:\n",
    "                    lo = v_\n",
    "                    mu_lo = mu_\n",
    "\n",
    "            raise Exception(\n",
    "                \"Error: no Mu range found to cross zero violation. Try wider Mu range, or smaller grid step.\"\n",
    "            )\n",
    "\n",
    "        mu_range = find_valid_mu_range(mu_range, step=0.5)\n",
    "        if np.isscalar(mu_range):\n",
    "            self.mu = mu_range\n",
    "        # elif len(mu_range) < 2:\n",
    "        #    self.mu = mu_range[0]\n",
    "        else:\n",
    "            mu0 = mu_range[0]\n",
    "            mu1 = mu_range[1]\n",
    "            self.mu = _binary_search_mu(mu0, mu1)\n",
    "            return self\n",
    "        _fit_given_mu(\n",
    "            X_src,\n",
    "            Y_src,\n",
    "            A_src,\n",
    "            src_st_ratio,\n",
    "            X_trg,\n",
    "            A_trg,\n",
    "            trg_st_ratio,\n",
    "            self.mu,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def _compute_p_and_q(self, theta, lambdas, mu, X, A, st_ratio):\n",
    "        z = _dot_intercept(theta, X)\n",
    "        n = X.shape[0]\n",
    "\n",
    "        def _solve_p_binary_search(mu, A, st_ratio, z, lambdas):\n",
    "            eps = self.lb_prob\n",
    "            a, b = np.zeros_like(z) + eps, np.ones_like(z) - eps\n",
    "            f = self.f_value(A)\n",
    "            mu_f = mu * f\n",
    "            b[mu_f > 1] = 1 / mu_f[mu_f > 1]\n",
    "            fun = (\n",
    "                lambda x: -np.log(1 / (1 - x))\n",
    "                + np.log(1 / x)\n",
    "                + mu * x * f\n",
    "                + st_ratio * z\n",
    "                + lambdas[0] * A\n",
    "                + lambdas[1] * (1 - A)\n",
    "            )\n",
    "            while np.any(abs(a - b) > self.tol):\n",
    "                m = (a + b) / 2\n",
    "                fc = fun(m)\n",
    "                a = np.where(fc >= 0, m, a)\n",
    "                b = np.where(fc <= 0, m, b)\n",
    "\n",
    "            return (a + b) / 2\n",
    "\n",
    "        p = _solve_p_binary_search(mu, A, st_ratio, z, lambdas)\n",
    "        q = p / (1 - mu * self.f_value(A) * p + mu * self.f_value(A) * np.square(p))\n",
    "        assert np.all(q >= 0)\n",
    "        assert np.all(q <= 1)\n",
    "\n",
    "        if not (np.all(q <= 1)):\n",
    "            print(\" Q <= 1 FAILED!\")\n",
    "        q = np.maximum(0, np.minimum(q, 1))\n",
    "        return p, q\n",
    "\n",
    "    def _sanitize_check_model(self):\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Missing model parameters for feature matching (theta).\")\n",
    "        if self.lambdas is None:\n",
    "            raise ValueError(\"Missing model parameters for group matching (lambdas).\")\n",
    "        if self.mu is None:\n",
    "            raise ValueError(\"Missing model parameters for fairness penalty (mu).\")\n",
    "\n",
    "    def _q_fairness_violation(self, p, q, A):\n",
    "        return (\n",
    "            (np.sum((p * q)[A == 1]) / self.trg_group_estimator(1))\n",
    "            - (np.sum((p * q)[A == 0]) / self.trg_group_estimator(0))\n",
    "        ) / p.shape[0]\n",
    "\n",
    "    def q_fairness_violation(self, X, A, st_ratio, mu=None):\n",
    "        if mu is None:\n",
    "            mu = self.mu\n",
    "        p, q = self._compute_p_and_q(self.theta, self.lambdas, mu, X, A, st_ratio)\n",
    "        return self._q_fairness_violation(p, q, A)\n",
    "\n",
    "    def _score(self, X, Y, A, st_ratio, mu, lambdas, theta):\n",
    "        p, _ = self._compute_p_and_q(theta, lambdas, mu, X, A, st_ratio)\n",
    "        return 1 - np.mean(abs(np.round(p) - Y))\n",
    "\n",
    "    def score(self, X, Y, A, st_ratio):\n",
    "        self._sanitize_check_model()\n",
    "        return self._score(X, Y, A, st_ratio, self.mu, self.lambdas, self.theta)\n",
    "\n",
    "    def expected_error(self, X, Y, A, ratio):\n",
    "        proba = self.predict_proba(X, A, ratio)\n",
    "        return np.mean(np.where(Y == 1, 1 - proba, proba))\n",
    "\n",
    "    def predict_proba(self, X, A, st_ratio):\n",
    "        self._sanitize_check_model()\n",
    "        p, _ = self._compute_p_and_q(self.theta, self.lambdas, self.mu, X, A, st_ratio)\n",
    "        return p\n",
    "\n",
    "    def predict(self, X, A, st_ratio):\n",
    "        return np.round(self.predict_proba(X, A, st_ratio))\n",
    "\n",
    "    def fairness_violation(self, X, Y, A, ratio):\n",
    "        proba = self.predict_proba(X, A, ratio)\n",
    "        return np.mean(proba[np.logical_and(Y == 1, A == 1)]) - np.mean(\n",
    "            proba[np.logical_and(Y == 1, A == 0)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1eb3fb8c-4b68-4cee-a174-d775b4a58756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDEAdapter:\n",
    "    def __init__(self, kde=NaiveKDE(kernel=\"gaussian\", bw=0.3)):\n",
    "        self._kde = kde\n",
    "\n",
    "    def fit(self, sample):\n",
    "        self._kde.fit(sample)\n",
    "        return self\n",
    "\n",
    "    def pdf(self, sample):\n",
    "        density = self._kde.evaluate(sample)\n",
    "        return density\n",
    "\n",
    "    def p(self, sample, eps=0):\n",
    "        density = self._kde.evaluate(sample)\n",
    "        return (density + eps) / np.sum(density + eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4598f61-df3a-4334-af6a-1d0226eed0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdebw = 0.3\n",
    "eps = 0.001\n",
    "\n",
    "src_kde = KDEAdapter(kde=NaiveKDE(kernel=\"gaussian\", bw=kdebw)).fit(\n",
    "    X_src\n",
    ")\n",
    "trg_kde = KDEAdapter(kde=NaiveKDE(kernel=\"gaussian\", bw=kdebw)).fit(\n",
    "    X_trg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f56b21ff-53ee-48f2-9da1-5c34f4927e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m src_kde\u001b[38;5;241m.\u001b[39mp(X_src,eps)\n",
      "Cell \u001b[0;32mIn[49], line 14\u001b[0m, in \u001b[0;36mKDEAdapter.p\u001b[0;34m(self, sample, eps)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kde\u001b[38;5;241m.\u001b[39mevaluate(sample)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (density \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(density \u001b[38;5;241m+\u001b[39m eps)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/KDEpy/NaiveKDE.py:134\u001b[0m, in \u001b[0;36mNaiveKDE.evaluate\u001b[0;34m(self, grid_points)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m weight, data_point, bw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, bw):\n\u001b[1;32m    133\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_points \u001b[38;5;241m-\u001b[39m data_point\n\u001b[0;32m--> 134\u001b[0m     evaluated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(x, bw\u001b[38;5;241m=\u001b[39mbw, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evalate_return_logic(evaluated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_points)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.11/site-packages/KDEpy/kernel_funcs.py:308\u001b[0m, in \u001b[0;36mKernel.evaluate\u001b[0;34m(self, x, bw, norm)\u001b[0m\n\u001b[1;32m    302\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to solve for support numerically. Use a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel with finite support or scale data to smaller bw.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, bw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    Evaluate the kernel.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m        Must have shape (obs, ), or float.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# If x is a number, convert it to a length-1 NumPy vector\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src_kde.p(X_src,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0ab0d-1f5b-4f2a-9b7a-956973738c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
