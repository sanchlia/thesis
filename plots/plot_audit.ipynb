{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bin.dataset import Dataset\n",
    "from collections import defaultdict\n",
    "from bin.metrics import Metrics\n",
    "\n",
    "\n",
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"LR\": None, \n",
    "    \"Reduction\": None,\n",
    "    \"Reweight\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path):\n",
    "    \"\"\"Reads the config file and returns a dictionary.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            config = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Config file not found.\")\n",
    "        config = None\n",
    "    return config\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads the csv file and returns a dataframe.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"CSV file not found.\")\n",
    "        df = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_conf = \"configs/adult_noisy.json\"\n",
    "# exp_conf = \"configs/compas_noisy.json\"\n",
    "exp_conf = \"configs/synthetic_20_noisy.json\"\n",
    "# exp_conf = \"configs/income_noisy.json\"\n",
    "# exp_conf = \"configs/baseline_config.json\"\n",
    "\n",
    "EXP = read_config(exp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "try:\n",
    "    for name, value in EXP.items():\n",
    "        # if name in ['COMPAS_bias_0.1', 'COMPAS_balanced_0.1']: continue\n",
    "        # if name in ['income_balanced_0.1', 'income_balanced_0.3']: continue\n",
    "        # if not name in ['synthetic_20_balanced_0.1']: continue\n",
    "        datasets[name] = Dataset(value)\n",
    "        datasets[name].calculate_probabilities(\"fair\")\n",
    "        datasets[name].calculate_probabilities(\"emp\")\n",
    "        \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = ['bias', 'balanced', 'flip']\n",
    "levels = ['0.1','0.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audit_table(noises, levels, base_path, name):\n",
    "    cols = ['metric', 'LR', 'reduction', 'reweight']\n",
    "    df = []\n",
    "    for noise in noises:\n",
    "        for level in ['0.1','0.3']:\n",
    "            audit_df = pd.read_csv(os.path.join(base_path, f\"{name}_{noise}_{level}\", \"audit.csv\"))\n",
    "            meta = [noise, level]\n",
    "            for metric in ['accuracy', 'Equal_Opportunity','equalized_odds']:\n",
    "                for eval_type in [ 'noisy','fair_clean', 'emp_clean']:\n",
    "                    temp = [metric, eval_type]\n",
    "                    for m in ['LR', 'Reduction', 'Reweight']:\n",
    "                        temp_df = audit_df[(audit_df['model'] == m)]\n",
    "                        # temp.append((temp_df[temp_df['eval_type'] == eval_type][metric].reset_index() - temp_df[temp_df['eval_type'] == 'ground'][metric].reset_index())[metric].mean())\n",
    "                        temp.append(\n",
    "                            f\"{(temp_df[temp_df['eval_type'] == eval_type][metric].reset_index() - temp_df[temp_df['eval_type'] == 'ground'][metric].reset_index())[metric].mean():.3f} + {(temp_df[temp_df['eval_type'] == eval_type][metric].reset_index() - temp_df[temp_df['eval_type'] == 'ground'][metric].reset_index())[metric].std():.3f}\"\n",
    "                            )\n",
    "                    df.append( meta + temp)\n",
    "            # print(audit_df)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2adf8fc93cedafd578fe7dd39840acbd27ab9eaddec0fc7c286627d45824dbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
